# -*- coding: utf-8 -*-
"""Test_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tS3BgH_dSXEEOwE2ILj0ViwI0Tn_Ible
"""

import pandas as pd
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from tqdm.auto import tqdm  # Optional: for progress bar

# --- Configuration ---
CSV_FILE_PATH = '/content/product_reviews.csv'
REVIEW_COLUMN_NAME = 'review_text'  # Update if needed based on your CSV column

PREDEFINED_ASPECTS = [
    "price", "quality", "delivery", "seller", "customer service",
    "screen", "battery", "performance", "camera", "audio", "video", "packaging", "ease of use"
]

MODEL_NAME = "tnatvu/xlm-roberta-base-absa"  # Pre-trained ABSA model

SENTIMENT_MAP = {0: "Negative", 1: "Neutral", 2: "Positive"}
NUM_SENTIMENT_LABELS = len(SENTIMENT_MAP)

# --- Load Data ---
try:
    df = pd.read_csv(CSV_FILE_PATH)
    print(f"Successfully loaded data from {CSV_FILE_PATH}")
    print(f"DataFrame shape: {df.shape}")
    if REVIEW_COLUMN_NAME not in df.columns:
        raise ValueError(f"Column '{REVIEW_COLUMN_NAME}' not found in {CSV_FILE_PATH}")
    df = df.dropna(subset=[REVIEW_COLUMN_NAME]).reset_index(drop=True)
    print(f"DataFrame shape after dropping missing reviews: {df.shape}")
except Exception as e:
    print(f"Error: {e}")
    exit()

# --- Load Pre-trained Model and Tokenizer ---
try:
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=NUM_SENTIMENT_LABELS)
    print("Model loaded successfully.")
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    model.eval()
    print(f"Using device: {device}")
except Exception as e:
    print(f"Error loading model or tokenizer: {e}")
    exit()

# --- Perform ABSA (Aspect Sentiment Classification) ---
print(f"\nAnalyzing sentiment for aspects: {PREDEFINED_ASPECTS}")

absa_results = []

# Use tqdm for a progress bar
for index, row in tqdm(df.iterrows(), total=df.shape[0], desc="Processing Reviews"):
    review_text = str(row[REVIEW_COLUMN_NAME])  # Ensure it's a string

    for aspect in PREDEFINED_ASPECTS:
        # Format input for ABSA
        input_text = f"<s> {review_text} </s></s> {aspect} </s>"

        try:
            # Tokenize
            inputs = tokenizer(input_text, return_tensors="pt", padding=True, truncation=True, max_length=512)
            inputs = {key: val.to(device) for key, val in inputs.items()}

            # Predict
            with torch.no_grad():
                outputs = model(**inputs)

            # Get sentiment
            logits = outputs.logits
            predicted_class_id = logits.argmax(dim=-1).item()
            predicted_sentiment = SENTIMENT_MAP.get(predicted_class_id, "Unknown")

            # Store result
            result_row = row.to_dict()  # Include original row info
            result_row['analyzed_aspect'] = aspect
            result_row['predicted_aspect_sentiment'] = predicted_sentiment
            absa_results.append(result_row)

        except Exception as e:
            print(f"\nError processing review index {index} for aspect '{aspect}': {e}")
            result_row = row.to_dict()
            result_row['analyzed_aspect'] = aspect
            result_row['predicted_aspect_sentiment'] = "Error"
            absa_results.append(result_row)

# --- Store and Display Results ---
results_df = pd.DataFrame(absa_results)

# Reorder columns for better display
cols = results_df.columns.tolist()
if 'analyzed_aspect' in cols and 'predicted_aspect_sentiment' in cols and REVIEW_COLUMN_NAME in cols:
     cols.remove('analyzed_aspect')
     cols.remove('predicted_aspect_sentiment')
     cols.remove(REVIEW_COLUMN_NAME)
     cols = [REVIEW_COLUMN_NAME, 'analyzed_aspect', 'predicted_aspect_sentiment'] + cols
     results_df = results_df[cols]

print("\n--- ABSA Results Sample ---")
print(results_df.head())

# Save the results to a new CSV file
output_csv_path = 'product_review_absa_results.csv'
results_df.to_csv(output_csv_path, index=False)
print(f"\nFull results saved to {output_csv_path}")